{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUCbdz1LZwc9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "import multiprocessing"
      ],
      "metadata": {
        "id": "jrzzPOBMVZ99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/drive/MyDrive/jeju_ppodpo/path_to_files/TL01.zip.part0 > /content/drive/MyDrive/jeju_ppodpo/path_to_files/file1.zip\n",
        "!cat /content/drive/MyDrive/jeju_ppodpo/path_to_files/TL02.zip.part0 > /content/drive/MyDrive/jeju_ppodpo/path_to_files/file2.zip"
      ],
      "metadata": {
        "id": "TvcadwVGaQ_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/jeju_ppodpo/path_to_files/file1.zip -d /content/drive/MyDrive/jeju_ppodpo/path_to_extract/jeju1/\n",
        "!unzip /content/drive/MyDrive/jeju_ppodpo/path_to_files/file2.zip -d /content/drive/MyDrive/jeju_ppodpo/path_to_extract/jeju2/"
      ],
      "metadata": {
        "id": "HkXXTh-5wRar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_single_file(json_path):\n",
        "    text = Path(json_path).read_text(encoding='utf-8')\n",
        "    obj = json.loads(text)\n",
        "    sentences = obj['transcription']['sentences']\n",
        "    records = []\n",
        "    for sent in sentences:\n",
        "        records.append({\n",
        "            'dialect': sent['dialect'],\n",
        "            'standard': sent['standard']\n",
        "        })\n",
        "    return records\n",
        "\n",
        "def load_jeju_dataset_parallel(dir_paths, use_recursive=False,\n",
        "                               max_workers=None, executor_type='thread'):\n",
        "    all_records = []\n",
        "    for dir_path in dir_paths:\n",
        "        p = Path(dir_path)\n",
        "        json_files = list(p.rglob('*.json')) if use_recursive else list(p.glob('*.json'))\n",
        "        total = len(json_files)\n",
        "        if total == 0:\n",
        "            continue\n",
        "\n",
        "        if max_workers is None:\n",
        "            cpu_cnt = multiprocessing.cpu_count()\n",
        "            if executor_type == 'process':\n",
        "                max_workers_effective = cpu_cnt\n",
        "            else:\n",
        "                max_workers_effective = min(32, cpu_cnt * 5)\n",
        "        else:\n",
        "            max_workers_effective = max_workers\n",
        "\n",
        "        Executor = ProcessPoolExecutor if executor_type == 'process' else ThreadPoolExecutor\n",
        "        with Executor(max_workers=max_workers_effective) as executor:\n",
        "            for recs in tqdm(executor.map(process_single_file, json_files),\n",
        "                             total=total, desc=f\"Processing {p.name}\", unit=\"file\"):\n",
        "                all_records.extend(recs)\n",
        "\n",
        "    return pd.DataFrame(all_records, columns=['dialect', 'standard'])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_dir = '/content/drive/MyDrive/jeju_ppodpo/path_to_extract'\n",
        "    dirs = [f'{base_dir}/jeju1', f'{base_dir}/jeju2']\n",
        "    df_jeju = load_jeju_dataset_parallel(dirs, use_recursive=False,\n",
        "                                         max_workers=24,\n",
        "                                         executor_type='thread')\n",
        "    df_jeju.to_csv('/content/drive/MyDrive/jeju_ppodpo/path_to_extract/jeju_all.csv', index=False)"
      ],
      "metadata": {
        "id": "TwP7XEGvH0rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_jeju = pd.read_csv('/content/drive/MyDrive/jeju_ppodpo/path_to_extract/jeju_all.csv')"
      ],
      "metadata": {
        "id": "pSO0sQJdXw_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_jeju = df_jeju.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df_sft  = df_jeju.iloc[:20000].reset_index(drop=True)\n",
        "df_dpo  = df_jeju.iloc[20000:55000].reset_index(drop=True)\n",
        "df_test = df_jeju.iloc[55000:].reset_index(drop=True)\n",
        "\n",
        "save_dir = Path('/content/drive/MyDrive/jeju_ppodpo/data')\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df_sft.to_csv(save_dir / 'df_sft.csv',  index=False)\n",
        "df_dpo.to_csv(save_dir / 'df_dpo.csv',  index=False)\n",
        "df_test.to_csv(save_dir / 'df_test.csv', index=False)"
      ],
      "metadata": {
        "id": "wuUYnHLVSWyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-openai"
      ],
      "metadata": {
        "id": "zISNeCAPYvGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_dpo = pd.read_csv('/content/drive/MyDrive/jeju_ppodpo/data/df_dpo.csv')"
      ],
      "metadata": {
        "id": "tn4Yn1WyWOK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
        "\n",
        "# 모델 및 체인 구성\n",
        "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5)\n",
        "\n",
        "# 프롬프트 (불필요 출력 방지)\n",
        "human_template = (\n",
        "    \"제주 방언을 자연스러운 표준어로 번역하세요.\\n\"\n",
        "    \"반드시 번역된 문장만 출력하세요. '표준어:', 따옴표, 설명은 포함하지 마세요.\\n\"\n",
        "    \"제주 방언: {text}\"\n",
        ")\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
        "chain = chat_prompt | model | StrOutputParser()\n",
        "\n",
        "# 후처리 함수\n",
        "def clean_output(output: str) -> str:\n",
        "    output = output.strip()\n",
        "    output = re.sub(r'^표준어\\s*[:：]?\\s*', '', output)  # '표준어:' 제거\n",
        "    output = output.strip(' \"\\'\\n')  # 따옴표 및 공백 제거\n",
        "    return output\n",
        "\n",
        "# 번역 함수 (후처리 포함)\n",
        "def translate_and_clean(text: str) -> str:\n",
        "    try:\n",
        "        raw_output = chain.invoke({\"text\": text})\n",
        "        return clean_output(raw_output)\n",
        "    except Exception as e:\n",
        "        return f\"[ERROR] {e}\"\n",
        "\n",
        "# 병렬 처리로 번역 실행\n",
        "results = [None] * len(df_dpo)\n",
        "with ThreadPoolExecutor(max_workers=32) as executor:\n",
        "    futures = {executor.submit(translate_and_clean, row): i for i, row in enumerate(df_dpo[\"dialect\"])}\n",
        "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Translating\"):\n",
        "        idx = futures[future]\n",
        "        try:\n",
        "            results[idx] = future.result()\n",
        "        except Exception as e:\n",
        "            results[idx] = f\"[ERROR] {e}\"\n",
        "\n",
        "# 결과 저장\n",
        "df_dpo[\"rejected\"] = results\n"
      ],
      "metadata": {
        "id": "nSYJfepzX20N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rew  = df_dpo.iloc[:20000].reset_index(drop=True)\n",
        "df_rl  = df_dpo.iloc[20000:].reset_index(drop=True)\n",
        "\n",
        "save_dir = Path('/content/drive/MyDrive/jeju_ppodpo/data')\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df_dpo.to_csv(save_dir / 'df_dpo.csv',  index=False)\n",
        "df_rew.to_csv(save_dir / 'df_rew.csv',  index=False)\n",
        "df_rl.to_csv(save_dir / 'df_rl.csv', index=False)"
      ],
      "metadata": {
        "id": "dx9ulTvTZaw5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}